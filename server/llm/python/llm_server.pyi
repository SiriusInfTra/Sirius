from typing import List

class LLMRequest:
    request_id: int
    prompt: str
    max_tokens: int

class LLMRequestMetric:
    num_prompt_token: int
    num_output_token: int
    queue_ms: float
    prefill_ms: float
    decode_ms: float

    def __init__(
        self,
        num_prompt_token: int,
        num_output_token: int,
        queue_ms: float,
        prefill_ms: float,
        decode_ms: float,
    ) -> None:
        ...

class KVCachePoolStat:
    num_idle_blk_grps: int
    num_allocated_blk_grps: int

class LLMServer:
    def Init(self) -> None: ...
    def Shutdown(self) -> None: ...
    def GetLLMRequests(self, batch_size: int, timeout_ms: int, block: bool) -> List[LLMRequest]: ...
    def FinishLLMRequest(self, request_id: int, output: str, metric: LLMRequestMetric) -> None: ...
    def IsRunning(self) -> bool: ...

def info(msg: str) -> None: ...
def finish_llm_request(request_id: int, output: str, metric: LLMRequestMetric) -> None: ...
def is_running() -> bool: ...
def maybe_set_kv_cache_block_nbytes(
    block_size: int,
    num_layers: int, 
    num_heads: int, 
    head_size: int, 
    block_nbytes: int
) -> None: ...
def get_num_gpu_kv_cache_blocks() -> int: ...
def free_kv_cache_block(blk_idx: list) -> None: ...
def alloc_kv_cache_block(n: int) -> int: ...
def get_num_free_nbytes() -> int: ...
def get_kv_cache_mem_page_util() -> float: ...
def use_kv_cache_pool() -> bool: ...
def create_kv_cache() -> None: ...
def enable_dynamic_sm_partition() -> bool: ...
def set_num_required_tpc(tpc_num: int) -> None: ...
def set_num_available_tpc(tpc_num: int, stream: int) -> None: ...
def info_with_frame(msg: str) -> None: ...
def dinfo(msg: str) -> None: ...