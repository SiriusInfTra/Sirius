# defined in torch_col/cython/dist.pyx

from typing import List, Tuple, Optional

def wait_barrier(): ...

def send_msg(src_rank: int, msg: str): ...

def recv_msg(src_rank: int) -> str: ...

def init_dynamic_batch_distributor(batch_size: int, 
                                   input_batch_size: int,
                                   global_batch_size: int,
                                   lazy_distributing: bool): ...

class _DynamicBatchDistirbutor:
    _lazy_distributing: bool
    def get_batch(batch_size: int) -> Tuple[List[Tuple[int, int]], bool]: ...
    def finish_batch(batch_range_vec: List[Tuple[int, int]],
                     end_of_global_batch: bool) -> None: ...
    def abort_batch(batch_range_vec: List[Tuple[int, int]],
                    end_of_global_batch: bool) -> None: ...
    def vote_finish_last_micro_batch() -> bool: ...
    def vote_abort_last_micro_batch() -> None: ...
    def reset_last_micro_batch_finish_vote() -> None: ...
    def distribute_batch(check_num_unproced_samples: bool,
                         distribute_to_all: bool,
                         at_global_batch_begin: bool) -> None: ...
    def next_epoch() -> int: ...
    def next_global_batch() -> None: ...
    def get_global_batch_size() -> int: ...
    def get_num_global_batch_per_epoch() -> int: ...
    def get_num_proced_global_batch() -> int: ...


def init_train_performance_model() -> None: ...

class _PerfModel:
    def __init__(self) -> None: ...
    def record_thpt(batch_size: int, batch_time_ms: float) -> None: ...
    def get_thpt(batch_size: int) -> float: ...
    def get_thpt_vec(batch_sizes: List[int]) -> List[float]: ...

